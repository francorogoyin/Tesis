{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679cc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from scipy.stats import jarque_bera\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b49017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run \"5. Convertir variables.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Modelo_Lineal_Robusto(Variables_X, Variable_Y, Nombre_Variable, \n",
    "                                                   Criterio_Eliminacion='aic', \n",
    "                                                   Umbral_VIF=5.0,\n",
    "                                                   Alpha_Significancia=0.05,\n",
    "                                                   Detectar_Outliers=True,\n",
    "                                                   Iteracion=1, \n",
    "                                                   Variables_Originales=None,\n",
    "                                                   Historial_Proceso=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Implementa regresión lineal robusta con metodología estadística rigurosa\n",
    "    que incluye eliminación secuencial de variables, detección de \n",
    "    multicolinealidad mediante VIF, identificación de outliers influyentes\n",
    "    y selección de modelos basada en criterios de información múltiples.\n",
    "    \n",
    "    Esta función aplica un enfoque metodológicamente sólido para construir\n",
    "    modelos de regresión robustos mediante un proceso iterativo que:\n",
    "    \n",
    "    1. DETECCIÓN DE OUTLIERS: Identifica observaciones influyentes usando\n",
    "       Distancia de Cook, leverage y residuos studentizados externamente.\n",
    "    \n",
    "    2. MULTICOLINEALIDAD RIGUROSA: Detecta multicolinealidad usando Factor\n",
    "       de Inflación de Varianza (VIF) para todas las variables, no solo\n",
    "       variables dummy predefinidas.\n",
    "    \n",
    "    3. ELIMINACIÓN SECUENCIAL: Elimina una variable por iteración (la menos\n",
    "       contribuyente) permitiendo que la significancia se reajuste naturalmente.\n",
    "    \n",
    "    4. CRITERIOS MÚLTIPLES: Usa AIC, BIC y p-valores combinados para decisiones\n",
    "       de selección de variables más informadas.\n",
    "    \n",
    "    5. ERRORES ESTÁNDAR ROBUSTOS: Implementa corrección HC3 para \n",
    "       heterocedasticidad y autocorrelación.\n",
    "    \n",
    "    6. DIAGNÓSTICOS COMPLETOS: Incluye tests de normalidad, \n",
    "       heterocedasticidad y autocorrelación de residuos.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    Variables_X : pandas.DataFrame\n",
    "        DataFrame con variables independientes numéricas. Debe estar\n",
    "        completamente limpio sin valores faltantes ni variables categóricas\n",
    "        sin codificar.\n",
    "    \n",
    "    Variable_Y : pandas.Series\n",
    "        Variable dependiente numérica continua del mismo tamaño que Variables_X.\n",
    "        No debe contener valores faltantes o infinitos.\n",
    "    \n",
    "    Nombre_Variable : str\n",
    "        Nombre descriptivo de la variable dependiente para reportes y\n",
    "        documentación de resultados.\n",
    "    \n",
    "    Criterio_Eliminacion : str, opcional (default='aic')\n",
    "        Criterio principal para eliminación de variables:\n",
    "        - 'aic': Akaike Information Criterion (penaliza complejidad)\n",
    "        - 'bic': Bayesian Information Criterion (más conservador)\n",
    "        - 'pvalue': Solo significancia estadística tradicional\n",
    "        - 'combinado': Usa AIC + p-valor simultáneamente\n",
    "    \n",
    "    Umbral_VIF : float, opcional (default=5.0)\n",
    "        Umbral máximo para Factor de Inflación de Varianza.\n",
    "        Variables con VIF > Umbral_VIF serán eliminadas por multicolinealidad.\n",
    "        Valores típicos: 5.0 (moderado), 10.0 (liberal), 2.5 (conservador).\n",
    "    \n",
    "    Alpha_Significancia : float, opcional (default=0.05)\n",
    "        Nivel de significancia para pruebas estadísticas y selección\n",
    "        de variables. Típicamente 0.05, 0.01 o 0.10.\n",
    "    \n",
    "    Detectar_Outliers : bool, opcional (default=True)\n",
    "        Si True, identifica y reporta observaciones influyentes usando\n",
    "        Distancia de Cook > 4/n, leverage > 2p/n y residuos studentizados\n",
    "        externos > 3. No las elimina automáticamente.\n",
    "    \n",
    "    Iteracion : int, opcional (default=1)\n",
    "        Parámetro interno para recursión. No modificar manualmente.\n",
    "    \n",
    "    Variables_Originales : list, opcional (default=None)\n",
    "        Lista interna de variables del modelo inicial. Se inicializa\n",
    "        automáticamente en la primera iteración.\n",
    "    \n",
    "    Historial_Proceso : list, opcional (default=None)\n",
    "        Lista interna que registra el historial de eliminaciones y\n",
    "        métricas en cada iteración para análisis post-hoc.\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    dict\n",
    "        Diccionario completo con resultados del modelo final robusto:\n",
    "        \n",
    "        INFORMACIÓN BÁSICA:\n",
    "        - 'Variable_Dependiente': Nombre de la variable dependiente\n",
    "        - 'N_Observaciones': Número de observaciones utilizadas\n",
    "        - 'N_Observaciones_Outliers': Observaciones identificadas como influyentes\n",
    "        \n",
    "        MÉTRICAS DE AJUSTE:\n",
    "        - 'R_Cuadrado': Coeficiente de determinación R²\n",
    "        - 'R_Cuadrado_Ajustado': R² ajustado por grados de libertad\n",
    "        - 'AIC': Akaike Information Criterion\n",
    "        - 'BIC': Bayesian Information Criterion\n",
    "        - 'F_Estadistico': Estadístico F global del modelo\n",
    "        - 'P_Valor_Modelo': P-valor del test F global\n",
    "        \n",
    "        VARIABLES Y COEFICIENTES:\n",
    "        - 'Variables_Significativas': Lista de predictores finales\n",
    "        - 'Variables_Eliminadas_Total': Variables removidas del modelo inicial\n",
    "        - 'Coeficientes': Diccionario con coeficientes estimados\n",
    "        - 'P_Valores': P-valores de cada coeficiente\n",
    "        - 'Intervalos_Confianza_95': Intervalos de confianza robustos\n",
    "        - 'VIF_Final': Factores de inflación de varianza finales\n",
    "        \n",
    "        PROCESO Y DIAGNÓSTICOS:\n",
    "        - 'Iteraciones_Totales': Número de iteraciones realizadas\n",
    "        - 'Criterio_Utilizado': Criterio de selección empleado\n",
    "        - 'Outliers_Detectados': Índices de observaciones influyentes\n",
    "        - 'Historial_Eliminaciones': Registro completo del proceso iterativo\n",
    "        \n",
    "        TESTS DIAGNÓSTICOS:\n",
    "        - 'Test_Normalidad_JB': Jarque-Bera test (estadístico, p-valor)\n",
    "        - 'Test_Heterocedasticidad_BP': Breusch-Pagan test (estadístico, p-valor)\n",
    "        - 'Durbin_Watson': Test de autocorrelación\n",
    "        - 'Numero_Condicion': Número de condición de la matriz X'X\n",
    "        \n",
    "        OBJETOS TÉCNICOS:\n",
    "        - 'Modelo_Objeto': Objeto statsmodels fitted model\n",
    "        - 'Matriz_Covarianza_Robusta': Matriz de covarianza HC3\n",
    "    \n",
    "    Ejemplo de uso:\n",
    "    ---------------\n",
    "    >>> # Modelo robusto con criterio AIC y detección de outliers\n",
    "    >>> Resultado = Modelo_Lineal_Robusto(\n",
    "    ...     Variables_X=df[['Edad', 'Educacion', 'Experiencia', 'Ingresos']], \n",
    "    ...     Variable_Y=df['Satisfaccion'],\n",
    "    ...     Nombre_Variable='Satisfaccion_Laboral',\n",
    "    ...     Criterio_Eliminacion='aic',\n",
    "    ...     Umbral_VIF=5.0,\n",
    "    ...     Detectar_Outliers=True\n",
    "    ... )\n",
    "    \n",
    "    >>> # Examinar resultados clave\n",
    "    >>> print(f\"Variables finales: {Resultado['Variables_Significativas']}\")\n",
    "    >>> print(f\"R² ajustado: {Resultado['R_Cuadrado_Ajustado']:.4f}\")\n",
    "    >>> print(f\"AIC: {Resultado['AIC']:.2f}\")\n",
    "    >>> print(f\"Outliers detectados: {len(Resultado['Outliers_Detectados'])}\")\n",
    "    \n",
    "    >>> # Acceso a diagnósticos detallados\n",
    "    >>> if Resultado['Test_Normalidad_JB'][1] < 0.05:\n",
    "    ...     print(\"⚠️ Violación del supuesto de normalidad\")\n",
    "    >>> if any(vif > 5 for vif in Resultado['VIF_Final'].values()):\n",
    "    ...     print(\"⚠️ Posible multicolinealidad residual\")\n",
    "    \n",
    "    Notas metodológicas:\n",
    "    --------------------\n",
    "    - ELIMINACIÓN SECUENCIAL: A diferencia de métodos que eliminan múltiples\n",
    "      variables simultáneamente, este enfoque elimina una por vez, permitiendo\n",
    "      que los efectos se reajusten naturalmente en cada iteración.\n",
    "    \n",
    "    - ROBUSTEZ ESTADÍSTICA: Los errores estándar HC3 son robustos a\n",
    "      heterocedasticidad y proporcionan inferencia válida incluso cuando\n",
    "      se violan supuestos de homocedasticidad.\n",
    "    \n",
    "    - SELECCIÓN DE MODELOS: AIC penaliza la complejidad del modelo balanceando\n",
    "      bondad de ajuste con parsimonia. BIC es más conservador y tiende a\n",
    "      seleccionar modelos más simples.\n",
    "    \n",
    "    - MULTICOLINEALIDAD: VIF > 5 indica multicolinealidad moderada, VIF > 10\n",
    "      indica multicolinealidad severa. Variables con VIF alto son eliminadas\n",
    "      antes de la selección principal.\n",
    "    \n",
    "    - OUTLIERS: Se detectan pero no se eliminan automáticamente para preservar\n",
    "      la integridad de los datos. El usuario debe evaluar si removerlos basándose\n",
    "      en contexto sustantivo.\n",
    "    \n",
    "    - VALIDEZ DE SUPUESTOS: Los tests diagnósticos permiten evaluar violaciones\n",
    "      de supuestos fundamentales de regresión lineal (normalidad, \n",
    "      heterocedasticidad, independencia).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializar parámetros en primera iteración.\n",
    "    if Variables_Originales is None:\n",
    "        Variables_Originales = Variables_X.columns.tolist()\n",
    "    \n",
    "    if Historial_Proceso is None:\n",
    "        Historial_Proceso = []\n",
    "    \n",
    "    print(f\"\\n{'='*25} ITERACIÓN {Iteracion} {'='*25}\")\n",
    "    print(f\"MODELO ROBUSTO METODOLÓGICAMENTE SÓLIDO: {Nombre_Variable}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # PASO 1: Verificar datos de entrada.\n",
    "    if len(Variables_X.columns) == 0:\n",
    "        print(\"\\n❌ ERROR: No quedan variables independientes\")\n",
    "        return None\n",
    "    \n",
    "    if len(Variables_X) != len(Variable_Y):\n",
    "        print(f\"\\n❌ ERROR: Dimensiones inconsistentes X({len(Variables_X)}) vs Y({len(Variable_Y)})\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Variables en modelo actual: {len(Variables_X.columns)}\")\n",
    "    print(f\"Observaciones disponibles: {len(Variables_X)}\")\n",
    "    \n",
    "    # PASO 2: Detección de outliers influyentes (solo informativo).\n",
    "    Indices_Outliers = []\n",
    "    if Detectar_Outliers and Iteracion == 1:\n",
    "        print(f\"\\n🔍 DETECCIÓN DE OUTLIERS INFLUYENTES:\")\n",
    "        \n",
    "        # Modelo preliminar para calcular influence measures.\n",
    "        Variables_X_Temp = sm.add_constant(Variables_X)\n",
    "        Modelo_Temp = sm.OLS(Variable_Y, Variables_X_Temp).fit()\n",
    "        Influence = OLSInfluence(Modelo_Temp)\n",
    "        \n",
    "        # Criterios múltiples para outliers.\n",
    "        Cooks_Distance = Influence.cooks_distance[0]\n",
    "        Leverage = Influence.hat_matrix_diag\n",
    "        Residuos_Studentizados = Influence.resid_studentized_external\n",
    "        \n",
    "        # Umbrales estadísticos estándar.\n",
    "        N_Obs = len(Variables_X)\n",
    "        P_Vars = len(Variables_X.columns) + 1  # +1 por constante\n",
    "        Umbral_Cooks = 4.0 / N_Obs\n",
    "        Umbral_Leverage = 2.0 * P_Vars / N_Obs\n",
    "        Umbral_Residuos = 3.0\n",
    "        \n",
    "        # Identificar outliers por cada criterio.\n",
    "        Outliers_Cooks = np.where(Cooks_Distance > Umbral_Cooks)[0]\n",
    "        Outliers_Leverage = np.where(Leverage > Umbral_Leverage)[0]\n",
    "        Outliers_Residuos = np.where(np.abs(Residuos_Studentizados) > Umbral_Residuos)[0]\n",
    "        \n",
    "        # Unión de todos los criterios.\n",
    "        Indices_Outliers = np.unique(np.concatenate([\n",
    "            Outliers_Cooks, Outliers_Leverage, Outliers_Residuos\n",
    "        ])).tolist()\n",
    "        \n",
    "        print(f\"  Outliers por Distancia de Cook (>{Umbral_Cooks:.4f}): {len(Outliers_Cooks)}\")\n",
    "        print(f\"  Outliers por Leverage (>{Umbral_Leverage:.4f}): {len(Outliers_Leverage)}\")  \n",
    "        print(f\"  Outliers por Residuos (>|{Umbral_Residuos}|): {len(Outliers_Residuos)}\")\n",
    "        print(f\"  Total outliers únicos detectados: {len(Indices_Outliers)}\")\n",
    "        \n",
    "        if len(Indices_Outliers) > 0:\n",
    "            print(f\"  ⚠️  {len(Indices_Outliers)} observaciones influyentes detectadas\")\n",
    "            print(\"  ℹ️  No se eliminan automáticamente - evalúe manualmente\")\n",
    "    \n",
    "    # PASO 3: Detección y eliminación de multicolinealidad con VIF.\n",
    "    print(f\"\\n📊 DETECCIÓN DE MULTICOLINEALIDAD (VIF):\")\n",
    "    \n",
    "    Variables_X_VIF = Variables_X.copy()\n",
    "    Variables_Eliminadas_VIF = []\n",
    "    \n",
    "    # Calcular VIF iterativamente hasta que todos sean < umbral.\n",
    "    while True:\n",
    "        if len(Variables_X_VIF.columns) < 2:\n",
    "            break\n",
    "            \n",
    "        # Calcular VIF para todas las variables actuales.\n",
    "        Variables_X_Con_Constante = sm.add_constant(Variables_X_VIF)\n",
    "        VIF_Valores = {}\n",
    "        \n",
    "        for i, Variable in enumerate(Variables_X_VIF.columns):\n",
    "            try:\n",
    "                VIF = variance_inflation_factor(Variables_X_Con_Constante.values, i + 1)\n",
    "                VIF_Valores[Variable] = VIF\n",
    "            except:\n",
    "                VIF_Valores[Variable] = float('inf')\n",
    "        \n",
    "        # Encontrar variable con mayor VIF.\n",
    "        Max_VIF_Variable = max(VIF_Valores.keys(), key=lambda x: VIF_Valores[x])\n",
    "        Max_VIF_Valor = VIF_Valores[Max_VIF_Variable]\n",
    "        \n",
    "        if Max_VIF_Valor <= Umbral_VIF:\n",
    "            break\n",
    "        \n",
    "        # Eliminar variable con mayor VIF.\n",
    "        print(f\"  Eliminando {Max_VIF_Variable} (VIF = {Max_VIF_Valor:.2f})\")\n",
    "        Variables_Eliminadas_VIF.append(Max_VIF_Variable)\n",
    "        Variables_X_VIF = Variables_X_VIF.drop(columns=[Max_VIF_Variable])\n",
    "    \n",
    "    # Reportar VIFs finales.\n",
    "    if len(Variables_X_VIF.columns) >= 2:\n",
    "        Variables_X_Con_Constante = sm.add_constant(Variables_X_VIF)\n",
    "        VIF_Final = {}\n",
    "        for i, Variable in enumerate(Variables_X_VIF.columns):\n",
    "            try:\n",
    "                VIF_Final[Variable] = variance_inflation_factor(Variables_X_Con_Constante.values, i + 1)\n",
    "            except:\n",
    "                VIF_Final[Variable] = float('nan')\n",
    "        \n",
    "        Max_VIF_Final = max(VIF_Final.values()) if VIF_Final else 0\n",
    "        print(f\"  Variables restantes: {len(Variables_X_VIF.columns)}\")\n",
    "        print(f\"  VIF máximo final: {Max_VIF_Final:.2f}\")\n",
    "        print(f\"  Variables eliminadas por VIF: {len(Variables_Eliminadas_VIF)}\")\n",
    "    else:\n",
    "        VIF_Final = {}\n",
    "        print(f\"  ⚠️  Muy pocas variables para calcular VIF\")\n",
    "    \n",
    "    # Usar variables depuradas por VIF.\n",
    "    Variables_X_Finales = Variables_X_VIF.copy()\n",
    "    \n",
    "    # PASO 4: Ajustar modelo con errores estándar robustos.\n",
    "    if len(Variables_X_Finales.columns) == 0:\n",
    "        print(f\"\\n❌ ERROR: Todas las variables eliminadas por multicolinealidad\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\n🔧 AJUSTANDO MODELO ROBUSTO:\")\n",
    "    Variables_X_Con_Constante = sm.add_constant(Variables_X_Finales)\n",
    "    Modelo = sm.OLS(Variable_Y, Variables_X_Con_Constante).fit(cov_type='HC3')\n",
    "    \n",
    "    # Métricas del modelo.\n",
    "    print(f\"  N observaciones: {Modelo.nobs:.0f}\")\n",
    "    print(f\"  R²: {Modelo.rsquared:.4f}\")\n",
    "    print(f\"  R² ajustado: {Modelo.rsquared_adj:.4f}\")\n",
    "    print(f\"  AIC: {Modelo.aic:.2f}\")\n",
    "    print(f\"  BIC: {Modelo.bic:.2f}\")\n",
    "    print(f\"  F-estadístico: {Modelo.fvalue:.2f}\")\n",
    "    print(f\"  P-valor modelo: {Modelo.f_pvalue:.6f}\")\n",
    "    \n",
    "    # PASO 5: Evaluación de variables y criterios de eliminación.\n",
    "    P_Valores = Modelo.pvalues\n",
    "    Variables_Candidatas = [var for var in P_Valores.index if var != 'const']\n",
    "    \n",
    "    if len(Variables_Candidatas) == 0:\n",
    "        print(f\"\\n❌ No hay variables predictoras en el modelo\")\n",
    "        return None\n",
    "    \n",
    "    # Identificar variable a eliminar según criterio.\n",
    "    Variable_A_Eliminar = None\n",
    "    \n",
    "    if Criterio_Eliminacion == 'pvalue':\n",
    "        # Criterio tradicional: eliminar la menos significativa si p >= alpha.\n",
    "        P_Vals_Variables = {var: P_Valores[var] for var in Variables_Candidatas}\n",
    "        Peor_Variable = max(P_Vals_Variables.keys(), key=lambda x: P_Vals_Variables[x])\n",
    "        \n",
    "        if P_Vals_Variables[Peor_Variable] >= Alpha_Significancia:\n",
    "            Variable_A_Eliminar = Peor_Variable\n",
    "            Razon_Eliminacion = f\"p-valor = {P_Vals_Variables[Peor_Variable]:.6f} >= {Alpha_Significancia}\"\n",
    "    \n",
    "    elif Criterio_Eliminacion in ['aic', 'bic']:\n",
    "        # Criterio de información: evaluar mejora si eliminamos cada variable.\n",
    "        AIC_Actual = Modelo.aic if Criterio_Eliminacion == 'aic' else Modelo.bic\n",
    "        Mejor_Criterio = AIC_Actual\n",
    "        \n",
    "        for Variable_Test in Variables_Candidatas:\n",
    "            # Probar modelo sin esta variable.\n",
    "            Variables_Sin_Test = [v for v in Variables_Candidatas if v != Variable_Test]\n",
    "            \n",
    "            if len(Variables_Sin_Test) == 0:\n",
    "                continue\n",
    "                \n",
    "            X_Test = sm.add_constant(Variables_X_Finales[Variables_Sin_Test])\n",
    "            Modelo_Test = sm.OLS(Variable_Y, X_Test).fit(cov_type='HC3')\n",
    "            Criterio_Test = Modelo_Test.aic if Criterio_Eliminacion == 'aic' else Modelo_Test.bic\n",
    "            \n",
    "            # Menor AIC/BIC es mejor.\n",
    "            if Criterio_Test < Mejor_Criterio:\n",
    "                Mejor_Criterio = Criterio_Test\n",
    "                Variable_A_Eliminar = Variable_Test\n",
    "                Razon_Eliminacion = f\"{Criterio_Eliminacion.upper()} mejora: {AIC_Actual:.2f} → {Criterio_Test:.2f}\"\n",
    "    \n",
    "    elif Criterio_Eliminacion == 'combinado':\n",
    "        # Combinado: AIC + significancia.\n",
    "        P_Vals_Variables = {var: P_Valores[var] for var in Variables_Candidatas}\n",
    "        Variables_No_Sig = [var for var in Variables_Candidatas \n",
    "                           if P_Vals_Variables[var] >= Alpha_Significancia]\n",
    "        \n",
    "        if Variables_No_Sig:\n",
    "            # Si hay no significativas, usar AIC entre ellas.\n",
    "            AIC_Actual = Modelo.aic\n",
    "            Mejor_AIC = AIC_Actual\n",
    "            \n",
    "            for Variable_Test in Variables_No_Sig:\n",
    "                Variables_Sin_Test = [v for v in Variables_Candidatas if v != Variable_Test]\n",
    "                X_Test = sm.add_constant(Variables_X_Finales[Variables_Sin_Test])\n",
    "                Modelo_Test = sm.OLS(Variable_Y, X_Test).fit(cov_type='HC3')\n",
    "                \n",
    "                if Modelo_Test.aic < Mejor_AIC:\n",
    "                    Mejor_AIC = Modelo_Test.aic\n",
    "                    Variable_A_Eliminar = Variable_Test\n",
    "                    Razon_Eliminacion = f\"No significativa (p={P_Vals_Variables[Variable_Test]:.4f}) + AIC mejora\"\n",
    "    \n",
    "    # PASO 6: Mostrar estado actual de variables.\n",
    "    print(f\"\\n📋 ESTADO DE VARIABLES:\")\n",
    "    Variables_Significativas = [var for var in Variables_Candidatas \n",
    "                               if P_Valores[var] < Alpha_Significancia]\n",
    "    Variables_No_Significativas = [var for var in Variables_Candidatas \n",
    "                                  if P_Valores[var] >= Alpha_Significancia]\n",
    "    \n",
    "    print(f\"  Significativas (p < {Alpha_Significancia}): {len(Variables_Significativas)}\")\n",
    "    for Variable in Variables_Significativas:\n",
    "        Beta = Modelo.params[Variable]\n",
    "        P_Val = P_Valores[Variable]\n",
    "        print(f\"    ✅ {Variable:<30} β = {Beta:8.4f}, p = {P_Val:.6f}\")\n",
    "    \n",
    "    print(f\"  No significativas (p >= {Alpha_Significancia}): {len(Variables_No_Significativas)}\")\n",
    "    for Variable in Variables_No_Significativas:\n",
    "        Beta = Modelo.params[Variable]  \n",
    "        P_Val = P_Valores[Variable]\n",
    "        print(f\"    ❌ {Variable:<30} β = {Beta:8.4f}, p = {P_Val:.6f}\")\n",
    "    \n",
    "    # Registrar iteración actual en historial.\n",
    "    Historial_Actual = {\n",
    "        'Iteracion': Iteracion,\n",
    "        'Variables_Incluidas': Variables_Candidatas.copy(),\n",
    "        'Variables_Significativas': Variables_Significativas.copy(),\n",
    "        'R_Cuadrado_Ajustado': Modelo.rsquared_adj,\n",
    "        'AIC': Modelo.aic,\n",
    "        'BIC': Modelo.bic,\n",
    "        'Variable_Eliminada': Variable_A_Eliminar,\n",
    "        'Razon_Eliminacion': Razon_Eliminacion if Variable_A_Eliminar else None\n",
    "    }\n",
    "    Historial_Proceso.append(Historial_Actual)\n",
    "    \n",
    "    # PASO 7: Decidir siguiente acción.\n",
    "    if Variable_A_Eliminar is None:\n",
    "        # CASO BASE: Modelo final alcanzado.\n",
    "        print(f\"\\n🎯 MODELO FINAL ALCANZADO en iteración {Iteracion}\")\n",
    "        print(f\"Criterio '{Criterio_Eliminacion}' no sugiere más eliminaciones\")\n",
    "        \n",
    "        # Ejecutar diagnósticos finales.\n",
    "        print(f\"\\n🔬 DIAGNÓSTICOS FINALES:\")\n",
    "        \n",
    "        # Test de normalidad Jarque-Bera.\n",
    "        JB_Stat, JB_Pval = jarque_bera(Modelo.resid)\n",
    "        print(f\"  Jarque-Bera (normalidad): JB = {JB_Stat:.3f}, p = {JB_Pval:.6f}\")\n",
    "        \n",
    "        # Test de heterocedasticidad Breusch-Pagan.\n",
    "        from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "        BP_LM, BP_LM_Pval, BP_F, BP_F_Pval = het_breuschpagan(Modelo.resid, Variables_X_Con_Constante)\n",
    "        print(f\"  Breusch-Pagan (heterocedasticidad): LM = {BP_LM:.3f}, p = {BP_LM_Pval:.6f}\")\n",
    "        \n",
    "        # Durbin-Watson (autocorrelación).\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        DW_Stat = durbin_watson(Modelo.resid)\n",
    "        print(f\"  Durbin-Watson (autocorrelación): DW = {DW_Stat:.3f}\")\n",
    "        \n",
    "        # Número de condición.\n",
    "        Num_Condicion = np.linalg.cond(Variables_X_Con_Constante)\n",
    "        print(f\"  Número de condición: {Num_Condicion:.1f}\")\n",
    "        \n",
    "        # Resumen del proceso completo.\n",
    "        Variables_Eliminadas_Total = [var for var in Variables_Originales \n",
    "                                    if var not in Variables_Significativas]\n",
    "        \n",
    "        print(f\"\\n📊 RESUMEN PROCESO COMPLETO:\")\n",
    "        print(f\"  Variables originales: {len(Variables_Originales)}\")\n",
    "        print(f\"  Eliminadas por VIF: {len(Variables_Eliminadas_VIF)}\")\n",
    "        print(f\"  Eliminadas por selección: {len(Variables_Eliminadas_Total) - len(Variables_Eliminadas_VIF)}\")\n",
    "        print(f\"  Variables finales: {len(Variables_Significativas)}\")\n",
    "        print(f\"  Iteraciones: {Iteracion}\")\n",
    "        print(f\"  R² ajustado final: {Modelo.rsquared_adj:.4f}\")\n",
    "        print(f\"  Outliers detectados: {len(Indices_Outliers)}\")\n",
    "        \n",
    "        # Construir diccionario de resultados completo.\n",
    "        Coeficientes = Modelo.params.to_dict()\n",
    "        P_Valores_Dict = P_Valores.to_dict()\n",
    "        Intervalos_Confianza = Modelo.conf_int().to_dict()\n",
    "        \n",
    "        Resultados_Finales = {\n",
    "            # Información básica.\n",
    "            'Variable_Dependiente': Nombre_Variable,\n",
    "            'N_Observaciones': int(Modelo.nobs),\n",
    "            'N_Observaciones_Outliers': len(Indices_Outliers),\n",
    "            \n",
    "            # Métricas de ajuste.\n",
    "            'R_Cuadrado': Modelo.rsquared,\n",
    "            'R_Cuadrado_Ajustado': Modelo.rsquared_adj,\n",
    "            'AIC': Modelo.aic,\n",
    "            'BIC': Modelo.bic,\n",
    "            'F_Estadistico': Modelo.fvalue,\n",
    "            'P_Valor_Modelo': Modelo.f_pvalue,\n",
    "            \n",
    "            # Variables y coeficientes.\n",
    "            'Variables_Significativas': Variables_Significativas,\n",
    "            'Variables_Eliminadas_Total': Variables_Eliminadas_Total,\n",
    "            'Variables_Eliminadas_VIF': Variables_Eliminadas_VIF,\n",
    "            'Coeficientes': Coeficientes,\n",
    "            'P_Valores': P_Valores_Dict,\n",
    "            'Intervalos_Confianza_95': Intervalos_Confianza,\n",
    "            'VIF_Final': VIF_Final,\n",
    "            \n",
    "            # Proceso y metodología.\n",
    "            'Iteraciones_Totales': Iteracion,\n",
    "            'Criterio_Utilizado': Criterio_Eliminacion,\n",
    "            'Umbral_VIF_Usado': Umbral_VIF,\n",
    "            'Alpha_Significancia_Usado': Alpha_Significancia,\n",
    "            'Outliers_Detectados': Indices_Outliers,\n",
    "            'Historial_Eliminaciones': Historial_Proceso,\n",
    "            \n",
    "            # Tests diagnósticos.\n",
    "            'Test_Normalidad_JB': (JB_Stat, JB_Pval),\n",
    "            'Test_Heterocedasticidad_BP': (BP_LM, BP_LM_Pval),\n",
    "            'Durbin_Watson': DW_Stat,\n",
    "            'Numero_Condicion': Num_Condicion,\n",
    "            \n",
    "            # Objetos técnicos.\n",
    "            'Modelo_Objeto': Modelo,\n",
    "            'Matriz_Covarianza_Robusta': Modelo.cov_HC3\n",
    "        }\n",
    "        \n",
    "        return Resultados_Finales\n",
    "    \n",
    "    else:\n",
    "        # CASO RECURSIVO: Continuar eliminando variables.\n",
    "        print(f\"\\n➡️  ELIMINANDO VARIABLE: {Variable_A_Eliminar}\")\n",
    "        print(f\"   Razón: {Razon_Eliminacion}\")\n",
    "        \n",
    "        Variables_X_Nuevas = Variables_X_Finales.drop(columns=[Variable_A_Eliminar])\n",
    "        \n",
    "        print(f\"\\nProcediendo a iteración {Iteracion + 1}...\")\n",
    "        \n",
    "        return Modelo_Lineal_Robusto(\n",
    "            Variables_X_Nuevas, Variable_Y, Nombre_Variable,\n",
    "            Criterio_Eliminacion, Umbral_VIF, Alpha_Significancia,\n",
    "            Detectar_Outliers, Iteracion + 1, Variables_Originales, \n",
    "            Historial_Proceso\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
